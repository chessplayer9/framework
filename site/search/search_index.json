{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Introduction The OpenHAIV framework primarily focuses on deep learning-based computer vision tasks and currently supports the following: Supervised Learning Supervised learning is one of the most commonly used machine learning algorithms. In the field of computer vision, tasks such as image classification typically rely on labeled data for training. Labeled data refers to paired input data and corresponding target outputs (labels). Through these pairs, the model learns the mapping from input to output. During training, the model continuously adjusts its internal parameters to minimize the gap between predictions and actual labels. A fundamental training framework should support supervised learning, and Openhaiv allows the use of different deep learning models, datasets, and flexible hyperparameter tuning. Incremental Learning Incremental learning is an advanced machine learning paradigm designed to address the issue of catastrophic forgetting in machine learning. Specifically, incremental learning enables models to learn new knowledge while retaining and optimizing previously acquired knowledge. Currently, mainstream incremental learning methods can be categorized into three types: regularization-based methods, experience replay-based methods, and parameter isolation-based methods. The Openhaiv framework currently supports multiple incremental learning algorithms, including ALICE, FACT, and SAVC, allowing users to select and adjust them based on task requirements. Few-Shot Learning Few-shot learning is a common scenario in deep learning, where models must learn and make inferences effectively with only a small number of samples. Specifically, few-shot learning uses a limited number of samples during training (typically 5\u201310 samples per class). The OpenHAIV framework currently supports few-shot learning, allowing users to define models, datasets, and the number of samples per class in the few-shot stage. Few-shot learning often employs methods such as contrastive learning and augmentation techniques, which are well-supported in Openhaiv. Additionally, the framework is highly extensible, enabling the integration of new few-shot learning methods. Out-of-Distribution Detection Out-of-distribution detection is the process of identifying whether data deviates from the known data distribution. On one hand, OOD detection ensures that models can make reliable judgments when encountering inputs different from known classes, thereby improving system robustness. On the other hand, OOD detection is a crucial component of incremental object recognition in open environments. The Openhaiv framework currently supports multiple OOD detection algorithms, allowing flexible adjustments for different domains and tasks. Novel Category Discovery Novel category discovery refers to identifying and discovering new categories in data without pre-existing labeled classes. Currently, novel category discovery typically employs unsupervised clustering algorithms in machine learning. Unsupervised clustering groups datasets into clusters, and Openhaiv supports the use of the K-Means clustering algorithm for novel category discovery. K-Means partitions data into K clusters, with each cluster represented by its centroid (the center point of the cluster). The algorithm iteratively updates centroids to minimize the within-cluster squared distances.","title":"Introduction"},{"location":"#introduction","text":"The OpenHAIV framework primarily focuses on deep learning-based computer vision tasks and currently supports the following:","title":"Introduction"},{"location":"#supervised-learning","text":"Supervised learning is one of the most commonly used machine learning algorithms. In the field of computer vision, tasks such as image classification typically rely on labeled data for training. Labeled data refers to paired input data and corresponding target outputs (labels). Through these pairs, the model learns the mapping from input to output. During training, the model continuously adjusts its internal parameters to minimize the gap between predictions and actual labels. A fundamental training framework should support supervised learning, and Openhaiv allows the use of different deep learning models, datasets, and flexible hyperparameter tuning.","title":"Supervised Learning"},{"location":"#incremental-learning","text":"Incremental learning is an advanced machine learning paradigm designed to address the issue of catastrophic forgetting in machine learning. Specifically, incremental learning enables models to learn new knowledge while retaining and optimizing previously acquired knowledge. Currently, mainstream incremental learning methods can be categorized into three types: regularization-based methods, experience replay-based methods, and parameter isolation-based methods. The Openhaiv framework currently supports multiple incremental learning algorithms, including ALICE, FACT, and SAVC, allowing users to select and adjust them based on task requirements.","title":"Incremental Learning"},{"location":"#few-shot-learning","text":"Few-shot learning is a common scenario in deep learning, where models must learn and make inferences effectively with only a small number of samples. Specifically, few-shot learning uses a limited number of samples during training (typically 5\u201310 samples per class). The OpenHAIV framework currently supports few-shot learning, allowing users to define models, datasets, and the number of samples per class in the few-shot stage. Few-shot learning often employs methods such as contrastive learning and augmentation techniques, which are well-supported in Openhaiv. Additionally, the framework is highly extensible, enabling the integration of new few-shot learning methods.","title":"Few-Shot Learning"},{"location":"#out-of-distribution-detection","text":"Out-of-distribution detection is the process of identifying whether data deviates from the known data distribution. On one hand, OOD detection ensures that models can make reliable judgments when encountering inputs different from known classes, thereby improving system robustness. On the other hand, OOD detection is a crucial component of incremental object recognition in open environments. The Openhaiv framework currently supports multiple OOD detection algorithms, allowing flexible adjustments for different domains and tasks.","title":"Out-of-Distribution Detection"},{"location":"#novel-category-discovery","text":"Novel category discovery refers to identifying and discovering new categories in data without pre-existing labeled classes. Currently, novel category discovery typically employs unsupervised clustering algorithms in machine learning. Unsupervised clustering groups datasets into clusters, and Openhaiv supports the use of the K-Means clustering algorithm for novel category discovery. K-Means partitions data into K clusters, with each cluster represented by its centroid (the center point of the cluster). The algorithm iteratively updates centroids to minimize the within-cluster squared distances.","title":"Novel Category Discovery"},{"location":"config/","text":"config config.algorithms config.dataloader config.model config.pipeline","title":"config"},{"location":"config/#config","text":"","title":"config"},{"location":"config/#configalgorithms","text":"","title":"config.algorithms"},{"location":"config/#configdataloader","text":"","title":"config.dataloader"},{"location":"config/#configmodel","text":"","title":"config.model"},{"location":"config/#configpipeline","text":"","title":"config.pipeline"},{"location":"document/","text":"config config.algorithms config.dataloader config.model config.pipeline ncdia algorithms Includes incremental learning algorithms, new class discovery algorithms, and out-of-distribution detection algorithms, as well as supervised learning algorithms. ncd.autoncd.py Modules related to novel class discovery. AutoNCD (model, train_loader, test_loader, device=None, verbose=False) Class for novel class discovery. model train_loader test_loader device verbose ncdia.dataloader ncdia.inference ncdia.model ncdia.trainers ncdia.utils scripts","title":"Document"},{"location":"document/#config","text":"","title":"config"},{"location":"document/#configalgorithms","text":"","title":"config.algorithms"},{"location":"document/#configdataloader","text":"","title":"config.dataloader"},{"location":"document/#configmodel","text":"","title":"config.model"},{"location":"document/#configpipeline","text":"","title":"config.pipeline"},{"location":"document/#ncdia","text":"","title":"ncdia"},{"location":"document/#algorithms","text":"Includes incremental learning algorithms, new class discovery algorithms, and out-of-distribution detection algorithms, as well as supervised learning algorithms.","title":"algorithms"},{"location":"document/#ncdautoncdpy","text":"Modules related to novel class discovery.","title":"ncd.autoncd.py"},{"location":"document/#autoncd-model-train_loader-test_loader-devicenone-verbosefalse","text":"Class for novel class discovery. model train_loader test_loader device verbose","title":"AutoNCD (model, train_loader, test_loader, device=None, verbose=False)"},{"location":"document/#ncdiadataloader","text":"","title":"ncdia.dataloader"},{"location":"document/#ncdiainference","text":"","title":"ncdia.inference"},{"location":"document/#ncdiamodel","text":"","title":"ncdia.model"},{"location":"document/#ncdiatrainers","text":"","title":"ncdia.trainers"},{"location":"document/#ncdiautils","text":"","title":"ncdia.utils"},{"location":"document/#scripts","text":"","title":"scripts"},{"location":"methods/","text":"Support Methods for OpenHAIV Class-incremental learning Joint : update models using all the data from all classes. Finetune : baseline method which simply update model using current data. LwF : Learning without Forgetting. ECCV2016 [ paper ] EWC : Overcoming catastrophic forgetting in neural networks. PNAS 2017 [ paper ] iCaRL : Incremental Classifier and Representation Learning. CVPR 2017 [ paper ] BiC : Large Scale Incremental Learning. CVPR 2019 [ paper ] WA : Maintaining Discrimination and Fairness in Class Incremental Learning. CVPR 2020 [ paper ] DER : DER: Dynamically Expandable Representation for Class Incremental Learning. CVPR 2021 [ paper ] Coil : Co-Transport for Class-Incremental Learning. ACM MM 2021 [ paper ] FOSTER : Feature Boosting and Compression for Class-incremental Learning. ECCV 2022 [ paper ] SSRE : Self-Sustaining Representation Expansion for Non-Exemplar Class-Incremental Learning. CVPR2022 [ paper ] FeTrIL : Feature Translation for Exemplar-Free Class-Incremental Learning. WACV2023 [ paper ] MEMO : A Model or 603 Exemplars: Towards Memory-Efficient Class-Incremental Learning. ICLR 2023 Spotlight [ paper ] Out-of-Distribution Detection Unimodal Methods MSP : A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks. ICLR 2017 [ paper ] MDS : A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks. NeurIPS 2018 [ paper ] MLS : Scaling Out-of-Distribution Detection for Real-World Settings. ICML 2022 [ paper ] vim : ViM: Out-Of-Distribution with Virtual-logit Matching. CVPR 2022 [ paper ] DML : Decoupling MaxLogit for Out-of-Distribution Detection. CVPR 2023 [ paper ] ODIN : Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks. ICLR 2018 [ paper ] FDBD : Fast Decision Boundary based Out-of-Distribution Detector. ICML 2024 [ paper ] ASH : . [ paper ] CIDER : . [ paper ] CSI : . [ paper ] GODIN : . [ paper ] MCP : . [ paper ] OpenMax : . [ paper ] MCD : . [ paper ] NPOS : . [ paper ] React : . [ paper ] CLIP-based Methods MCM : Delving into Out-of-Distribution Detection with Vision-Language Representations. NeurIPS 2022[ paper ] NegLabel : Negative Label Guided OOD Detection with Pretrained Vision-Language Models. ICLR 2024[ paper ] CoOp : Learning to Prompt for Vision-Language Models. IJCV 2022[ paper ] LoCoOp : LoCoOp: Few-Shot Out-of-Distribution Detection via Prompt Learning. NeurIPS 2023[ paper ] SCT : Self-Calibrated Tuning of Vision-Language Models for Out-of-Distribution Detection. NeurIPS 2024[ paper ] Maple : MaPLe: Multi-modal Prompt Learning. CVPR 2023[ paper ] DPM : Vision-Language Dual-Pattern Matching for Out-of-Distribution Detection. ECCV 2024[ paper ] Tip-Adapter : Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling. ECCV 2022[ paper ] NegPrompt : Learning Transferable Negative Prompts for Out-of-Distribution Detection. CVPR 2024[ paper ] GL-MCM : GL-MCM: Global and Local Maximum Concept Matching for Zero-Shot Out-of-Distribution Detection. IJCV 2025 [ paper ] Few-shot class-incremental learning Alice : Few-Shot Class-Incremental Learning from an Open-Set Perspective. ECCV 2022 [ paper ] FACT : Forward Compatible Few-Shot Class-Incremental Learning. CVPR 2022 [ paper ] SAVC : Learning with Fantasy: Semantic-Aware Virtual Contrastive Constraint for Few-Shot Class-Incremental Learning. CVPR 2023 [ paper ]","title":"Reproduced Methods"},{"location":"methods/#support-methods-for-openhaiv","text":"","title":"Support Methods for OpenHAIV"},{"location":"methods/#class-incremental-learning","text":"Joint : update models using all the data from all classes. Finetune : baseline method which simply update model using current data. LwF : Learning without Forgetting. ECCV2016 [ paper ] EWC : Overcoming catastrophic forgetting in neural networks. PNAS 2017 [ paper ] iCaRL : Incremental Classifier and Representation Learning. CVPR 2017 [ paper ] BiC : Large Scale Incremental Learning. CVPR 2019 [ paper ] WA : Maintaining Discrimination and Fairness in Class Incremental Learning. CVPR 2020 [ paper ] DER : DER: Dynamically Expandable Representation for Class Incremental Learning. CVPR 2021 [ paper ] Coil : Co-Transport for Class-Incremental Learning. ACM MM 2021 [ paper ] FOSTER : Feature Boosting and Compression for Class-incremental Learning. ECCV 2022 [ paper ] SSRE : Self-Sustaining Representation Expansion for Non-Exemplar Class-Incremental Learning. CVPR2022 [ paper ] FeTrIL : Feature Translation for Exemplar-Free Class-Incremental Learning. WACV2023 [ paper ] MEMO : A Model or 603 Exemplars: Towards Memory-Efficient Class-Incremental Learning. ICLR 2023 Spotlight [ paper ]","title":"Class-incremental learning"},{"location":"methods/#out-of-distribution-detection","text":"","title":"Out-of-Distribution Detection"},{"location":"methods/#unimodal-methods","text":"MSP : A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks. ICLR 2017 [ paper ] MDS : A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks. NeurIPS 2018 [ paper ] MLS : Scaling Out-of-Distribution Detection for Real-World Settings. ICML 2022 [ paper ] vim : ViM: Out-Of-Distribution with Virtual-logit Matching. CVPR 2022 [ paper ] DML : Decoupling MaxLogit for Out-of-Distribution Detection. CVPR 2023 [ paper ] ODIN : Enhancing The Reliability of Out-of-distribution Image Detection in Neural Networks. ICLR 2018 [ paper ] FDBD : Fast Decision Boundary based Out-of-Distribution Detector. ICML 2024 [ paper ] ASH : . [ paper ] CIDER : . [ paper ] CSI : . [ paper ] GODIN : . [ paper ] MCP : . [ paper ] OpenMax : . [ paper ] MCD : . [ paper ] NPOS : . [ paper ] React : . [ paper ]","title":"Unimodal Methods"},{"location":"methods/#clip-based-methods","text":"MCM : Delving into Out-of-Distribution Detection with Vision-Language Representations. NeurIPS 2022[ paper ] NegLabel : Negative Label Guided OOD Detection with Pretrained Vision-Language Models. ICLR 2024[ paper ] CoOp : Learning to Prompt for Vision-Language Models. IJCV 2022[ paper ] LoCoOp : LoCoOp: Few-Shot Out-of-Distribution Detection via Prompt Learning. NeurIPS 2023[ paper ] SCT : Self-Calibrated Tuning of Vision-Language Models for Out-of-Distribution Detection. NeurIPS 2024[ paper ] Maple : MaPLe: Multi-modal Prompt Learning. CVPR 2023[ paper ] DPM : Vision-Language Dual-Pattern Matching for Out-of-Distribution Detection. ECCV 2024[ paper ] Tip-Adapter : Tip-Adapter: Training-free CLIP-Adapter for Better Vision-Language Modeling. ECCV 2022[ paper ] NegPrompt : Learning Transferable Negative Prompts for Out-of-Distribution Detection. CVPR 2024[ paper ] GL-MCM : GL-MCM: Global and Local Maximum Concept Matching for Zero-Shot Out-of-Distribution Detection. IJCV 2025 [ paper ]","title":"CLIP-based Methods"},{"location":"methods/#few-shot-class-incremental-learning","text":"Alice : Few-Shot Class-Incremental Learning from an Open-Set Perspective. ECCV 2022 [ paper ] FACT : Forward Compatible Few-Shot Class-Incremental Learning. CVPR 2022 [ paper ] SAVC : Learning with Fantasy: Semantic-Aware Virtual Contrastive Constraint for Few-Shot Class-Incremental Learning. CVPR 2023 [ paper ]","title":"Few-shot class-incremental learning"},{"location":"ncdia/","text":"ncdia ncdia.algorithms Includes incremental learning algorithms, new class discovery algorithms, and out-of-distribution detection algorithms, as well as supervised learning algorithms. ncdia.algorithms.base.py BaseAlg Basic algorithm class to define the interface of an algorithm. __init__(self, trainer) The constructor method that initializes an instance of BaseAlg . Parameters: trainer ( object ): Trainer object. train_step(self, trainer, data, label, *args, kwargs)** Training step. Parameters: trainer ( object ): Trainer object. data ( torch.Tensor ): Input data. label ( torch.Tensor ): Label data. args ( tuple ): Additional arguments. kwargs ( dict ): Additional keyword arguments. Returns: results ( dict ): Training results. Contains the following keys: \"loss\" : Loss value. \"acc\" : Accuracy value. other \"key:value\" pairs. val_step(self, trainer, data, label, *args, kwargs)** Validation step. Parameters: trainer ( object ): Trainer object. data ( torch.Tensor ): Input data. label ( torch.Tensor ): Label data. args ( tuple ): Additional arguments. kwargs ( dict ): Additional keyword arguments. Returns: results ( dict ): Validation results. Contains the following keys: \"loss\" : Loss value. \"acc\" : Accuracy value. other \"key:value\" pairs. test_step(self, trainer, data, label, *args, kwargs)** Test step. Parameters: trainer ( object ): Trainer object. data ( torch.Tensor ): Input data. label ( torch.Tensor ): Label data. args ( tuple ): Additional arguments. kwargs ( dict ): Additional keyword arguments. Returns: results ( dict ): Test results. Contains the following keys: \"loss\" : Loss value. \"acc\" : Accuracy value. other \"key:value\" pairs. ncdia.algorithms.incremental Include implementation of Class Incremental Learning (CIL) and Few-shot Class Incremental Learning (FSCIL) algorithm. CIL Finetune LwF EwC iCarL IL2A WA FSCIL ALICE FACT SAVC ncdia.algorithms.ncd.autoncd.py Modules related to novel class discovery. AutoNCD Class for evaluating with OOD metrics and relabeling the OOD dataset for the next session. __init__(self, model, train_loader, test_loader, device=None, verbose=False) The constructor method that initializes an instance of AutoNCD . Parameters: model ( nn.Module ): model to be evaluated. train_loader ( DataLoader ): train dataloader. test_loader ( DataLoader ): test dataloader. device ( torch.device, optional ): device to run the evaluation. Default to None. verbose ( bool, optional ): print the progress bar. Default to False. inference(self, dataloader, split='train') Inference the model on the dataloader and return relevant information. If split is 'train', return the prototype of the training data. Parameters: dataloader ( DataLoader ): dataloader for evaluation. split ( str, optional ): train or test. Defaults to 'train'. Returns: If split is 'train': features ( torch.Tensor ): feature vectors, (N, D). logits ( torch.Tensor ): logit vectors, (N, C). prototype_cls ( torch.Tensor ): prototype vectors, (C, D). If split is 'test': imgpaths ( list ): image paths (list). features ( torch.Tensor ): feature vectors, (N, D). logits ( torch.Tensor ): logit vectors, (N, C). preds ( torch.Tensor ): prediction labels, (N,). labels ( torch.Tensor ): ground truth labels, (N,). relabel(self, ood_loader, metrics=[], tpr_th=0.95, prec_th=None) Relabel the OOD dataset for the next session. Parameters: ood_loader ( DataLoader ): OOD dataloader for relabeling. metrics ( list, optional ): metrics to evaluate the OOD dataset. Defaults to []. tpr_th ( float, optional ): True positive rate threshold. Defaults to 0.95. prec_th ( float, optional ): Precision threshold. Defaults to None. Returns: ood_loader ( DataLoader ): relabeled OOD dataloader. _split_cluster_label(self, y_label, y_pred, ood_class) Calculate clustering accuracy. Require scikit-learn installed. First compute linear assignment on all data, then look at how good the accuracy is on subsets. Parameters: y_label ( numpy.array ): true labels, (n_samples,) y_pred ( numpy.array ): predicted labels (n_samples,) ood_class : out-of-distribution class labels Returns: cluster_label : cluster label search_discrete_point(self, novel_feat, novel_target) TODO ncdia.algorithms.ood.autoood.py AutoOOD Class for evaluating OOD detection methods. eval(prototype_cls, fc_weight, train_feats, train_logits, id_feats, id_logits, id_labels, ood_feats, ood_logits, ood_labels, metrics=[], tpr_th=0.95, prec_th=None, id_attrs=None, ood_attrs=None, prototype_att=None) Evaluate the OOD detection methods and return OOD scores. Parameters: prototype_cls ( np.ndarray ): prototype of training data fc_weight ( np.ndarray ): weight of the last layer train_feats ( np.ndarray ): feature of training data train_logits ( np.ndarray ): logits of training data id_feats ( np.ndarray ): feature of ID data id_logits ( np.ndarray ): logits of ID data id_labels ( np.ndarray ): labels of ID data ood_feats ( np.ndarray ): feature of OOD data ood_logits ( np.ndarray ): logits of OOD data ood_labels ( np.ndarray ): labels of OOD data metrics ( list, optional ): list of OOD detection methods to evaluate. Defaults to []. tpr_th ( float, optional ): True positive rate threshold. Defaults to 0.95. prec_th ( float, optional ): Precision threshold. Defaults to None. Returns: ood_scores ( dict ): OOD scores, keys are the names of the OOD detection methods, values are the OOD scores and search threshold. Each value is a tuple containing the following: ood metrics ( tuple ): fpr ( float ): false positive rate auroc ( float ): area under the ROC curve aupr_in ( float ): area under the precision-recall curve for in-distribution samples aupr_out ( float ): area under the precision-recall curve for out-of-distribution samples search threshold ( tuple ): threshold for OOD detection if prec_th is not None best_th ( float ): best threshold for OOD detection conf ( torch.Tensor ): confidence scores label ( torch.Tensor ): label array precisions ( float ): precision when precisions >= prec_th recalls ( float ): recall when precisions >= prec_th inference(metrics, logits, feat, train_logits, train_feat, fc_weight, prototype, logits_att=None, prototype_att=None) Inferencec method for OOD detection Parameters: metrics ( list ): the ood metrics used for inference. logits ( np.ndarray ): logits of inference data. feat ( np.ndarray ): features of inference data. train_logits ( np.ndarray ): logits of training data. train_feat ( np.ndarray ): features of training data. fc_weight ( np.ndarray ): weight of the last layer. prototype ( np.ndarray ): prototypes of training data. logits_att ( np.ndarray, optional ): logits of attribute. prototype_att ( np.ndarray, optional ): prototypes of attribute. Returns: conf ( dict ): contains the confidence using different metrics, conf[metric] ( torch.Tensor ) is the confidence using specific metric. ncdia.algorithms.ood.methods.py msp(id_gt, id_logits, ood_gt, ood_logits, tpr_th=0.95, prec_th=None) Maximum Softmax Probability (MSP) method for OOD detection. A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks. Parameters: id_gt ( torch.Tensor ): ID ground truth labels. Shape (N,). id_logits ( torch.Tensor ): ID logits. Shape (N, C). ood_gt ( torch.Tensor ): OOD ground truth labels. Shape (M,). ood_logits ( torch.Tensor ): OOD logits. Shape (M, C). tpr_th ( float ): True positive rate threshold to compute false positive rate. Default is 0.95. prec_th ( float ): Precision threshold for searching - threshold. If None, not searching for threshold. Default is None. Returns: conf ( np.ndarray ): Confidence scores. Shape (N + M,). label ( np.ndarray ): Label array. Shape (N + M,). fpr ( float ): False positive rate. auroc ( float ): Area under the ROC curve. aupr_in ( float ): Area under the precision-recall curve for in-distribution samples. aupr_out ( float ): Area under the precision-recall curve for out-of-distribution best_th ( float ): Threshold for OOD detection. If prec_th is None, None. prec ( float ): Precision at the threshold. If prec_th is None, None. recall ( float ): Recall at the threshold. If prec_th is None, None. mcm(id_gt, id_logits, ood_gt, ood_logits, T=2, tpr_th=0.95, prec_th=None) Maximum Concept Matching (MCM) method for OOD detection. Delving into Out-of-Distribution Detection with Vision-Language Representations Parameters: id_gt ( torch.Tensor ): ID ground truth labels. Shape (N,). id_logits ( torch.Tensor ): ID logits. Shape (N, C). ood_gt ( torch.Tensor ): OOD ground truth labels. Shape (M,). ood_logits ( torch.Tensor ): OOD logits. Shape (M, C). T ( int ): Temperature for softmax. tpr_th ( float ): True positive rate threshold to compute false positive rate. Default is 0.95. prec_th ( float ): Precision threshold for searching threshold. If None, not searching for threshold. Default is None. Returns: fpr ( float ): False positive rate. auroc ( float ): Area under the ROC curve. aupr_in ( float ): Area under the precision-recall curve for in-distribution samples. aupr_out ( float ): Area under the precision-recall curve for out-of-distribution. max_logit(id_gt, id_logits, ood_gt, ood_logits, tpr_th=0.95, prec_th=None) Maximum Logit (MaxLogit) method for OOD detection. Scaling Out-of-Distribution Detection for Real-World Settings Parameters: id_gt ( torch.Tensor ): ID ground truth labels. Shape (N,). id_logits ( torch.Tensor ): ID logits. Shape (N, C). ood_gt ( torch.Tensor ): OOD ground truth labels. Shape (M,). ood_logits ( torch.Tensor ): OOD logits. Shape (M, C). tpr_th ( float ): True positive rate threshold to compute false positive rate. Default is 0.95. prec_th ( float ): Precision threshold for searching threshold. If None, not searching for threshold. Default is None. Returns: fpr ( float ): False positive rate. auroc ( float ): Area under the ROC curve. aupr_in ( float ): Area under the precision-recall curve for in-distribution samples. aupr_out ( float ): Area under the precision-recall curve for out-of-distribution energy(id_gt, id_logits, ood_gt, ood_logits, tpr_th=0.95, prec_th=None) Energy-based method for OOD detection. Energy-based Out-of-distribution Detection Parameters: id_gt ( torch.Tensor ): ID ground truth labels. Shape (N,). id_logits ( torch.Tensor ): ID logits. Shape (N, C). ood_gt ( torch.Tensor ): OOD ground truth labels. Shape (M,). ood_logits ( torch.Tensor ): OOD logits. Shape (M, C). tpr_th ( float ): True positive rate threshold to compute false positive rate. Default is 0.95. prec_th ( float ): Precision threshold for searching threshold. If None, not searching for threshold. Default is None. Returns: fpr ( float ): False positive rate. auroc ( float ): Area under the ROC curve. aupr_in ( float ): Area under the precision-recall curve for in-distribution samples. aupr_out ( float ): Area under the precision-recall curve for out-of-distribution vim(id_gt, id_logits, id_feat, ood_gt, ood_logits, ood_feat, train_logits, train_feat, tpr_th=0.95, prec_th=None) Virtual-Logit Matching (ViM) method for OOD detection. ViM: Out-of-Distribution With Virtual-Logit Matching Parameters: id_gt ( torch.Tensor ): ID ground truth labels. Shape (N,). id_logits ( torch.Tensor ): ID logits. Shape (N, C). id_feat ( torch.Tensor ): ID features. Shape (N, D). ood_gt ( torch.Tensor ): OOD ground truth labels. Shape (M,). ood_logits ( torch.Tensor ): OOD logits. Shape (M, C). ood_feat ( torch.Tensor ): OOD features. Shape (M, D). train_logits ( torch.Tensor ): Training logits. Shape (K, C). train_feat ( torch.Tensor ): Training features. Shape (K, D). tpr_th ( float ): True positive rate threshold to compute false positive rate. Default is 0.95. prec_th ( float ): Precision threshold for searching threshold. If None, not searching for threshold. Default is None. Returns: fpr ( float ): False positive rate. auroc ( float ): Area under the ROC curve. aupr_in ( float ): Area under the precision-recall curve for in-distribution samples. aupr_out ( float ): Area under the precision-recall curve for out-of-distribution dml(id_gt, id_feat, ood_gt, ood_feat, fc_weight, tpr_th=0.95, prec_th=None) Decoupled MaxLogit (DML) method for OOD detection. Decoupling MaxLogit for Out-of-Distribution Detection Parameters: id_gt ( torch.Tensor ): ID ground truth labels. Shape (N,). id_feat ( torch.Tensor ): ID features. Shape (N, D). ood_gt ( torch.Tensor ): OOD ground truth labels. Shape (M,). ood_feat ( torch.Tensor ): OOD features. Shape (M, D). fc_weight ( torch.Tensor ): FC layer weight. Shape (C, D). tpr_th ( float ): True positive rate threshold to compute false positive rate. Default is 0.95. prec_th ( float ): Precision threshold for searching threshold. If None, not searching for threshold. Default is None. Returns: fpr ( float ): False positive rate. auroc ( float ): Area under the ROC curve. aupr_in ( float ): Area under the precision-recall curve for in-distribution samples. aupr_out ( float ): Area under the precision-recall curve for out-of-distribution dmlp(id_gt, id_logits, id_feat, ood_gt, ood_logits, ood_feat, fc_weight, prototype,tpr_th=0.95, prec_th=None) Decoupled MaxLogit+ (DML+) method for OOD detection. Decoupling MaxLogit for Out-of-Distribution Detection Parameters: id_gt ( torch.Tensor ): ID ground truth labels. Shape (N,). id_logits ( torch.Tensor ): ID logits. Shape (N, C). id_feat ( torch.Tensor ): ID features. Shape (N, D). ood_gt ( torch.Tensor ): OOD ground truth labels. Shape (M,). ood_logits ( torch.Tensor ): OOD logits. Shape (M, C). ood_feat ( torch.Tensor ): OOD features. Shape (M, D). fc_weight ( torch.Tensor ): FC layer weight. Shape (D, C). prototype ( torch.Tensor ): Prototype. Shape (D, C). tpr_th ( float ): True positive rate threshold to compute false positive rate. Default is 0.95. prec_th ( float ): Precision threshold for searching threshold. If None, not searching for threshold. Default is None. Returns: fpr ( float ): False positive rate. auroc ( float ): Area under the ROC curve. aupr_in ( float ): Area under the precision-recall curve for in-distribution samples. aupr_out ( float ): Area under the precision-recall curve for out-of-distribution prot(id_gt, id_logits, ood_gt, ood_logits, prototypes: list, tpr_th=0.95, prec_th=None) Prototype-based (Prot) method for OOD detection. Parameters: id_gt ( torch.Tensor ): ID ground truth labels, shape (N,). id_logits ( list of torch.Tensor ): ID logits, containing shape (N, C). ood_gt ( torch.Tensor ): OOD ground truth labels, shape (M,). ood_logits ( list of torch.Tensor ): OOD logits, containing shape (M, C). prototypes ( list of torch.Tensor ): Prototypes, containing shape (D, C). tpr_th ( float ): True positive rate threshold to compute false positive rate. prec_th ( float ): Precision threshold for searching threshold. If None, not searching for threshold. Default is Returns: fpr ( float ): False positive rate. auroc ( float ): Area under the ROC curve. aupr_in ( float ): Area under the precision-recall curve for in-distribution samples. aupr_out ( float ): Area under the precision-recall curve for out-of-distribution ncdia.algorithms.ood.inference.py The inference version of implemented ood methods in ncdia.algorithms.ood.methods.py ncdia.algorithms.ood.metrics.py ood_metrics(conf, label, tpr_th=0.95) Compute OOD metrics. Parameters: conf ( np.ndarray ): Confidence scores. Shape (N,). label ( np.ndarray ): Label array. Shape (N,). Containing: -1: OOD samples. int >= 0: ID samples with class labels tpr_th ( float ): True positive rate threshold to compute false positive rate. Returns: fpr ( float ): False positive rate. auroc ( float ): Area under the ROC curve. aupr_in ( float ): Area under the precision-recall curve for in-distribution samples. aupr_out ( float ): Area under the precision-recall curve for out-of-distribution samples. search_threshold(conf, label, prec_th) Search for the threshold for OOD detection. Parameters: conf ( np.ndarray ): Confidence scores. Shape (N,). label ( np.ndarray ): Label array. Shape (N,). Containing: -1: OOD samples. int >= 0: ID samples with class labels prec_th ( float ): Precision threshold. Returns: best_th ( float ): Threshold for OOD detection. prec ( float ): Precision at the threshold. recall ( float ): Recall at the threshold. ncdia.algorithms.supervised.standard.py Modules related to supervised learning StandardSL Class inherits from BaseAlg . Standard supervised learning algorithm train_step(self, trainer, data, label, *args, **kwargs) Training step for standard supervised learning. Parameters: trainer ( object ): Trainer object. data ( torch.Tensor ): Input data. label ( torch.Tensor ): Label data. args ( tuple ): Additional arguments. kwargs ( dict ): Additional keyword arguments. Returns: results ( dict ): Training results. Contains the following keys: \"loss\" : Loss value. \"acc\" : Accuracy value. val_step(self, trainer, data, label, *args, **kwargs) Validation step for standard supervised learning. Parameters: trainer ( object ): Trainer object. data ( torch.Tensor ): Input data. label ( torch.Tensor ): Label data. args ( tuple ): Additional arguments. kwargs ( dict ): Additional keyword arguments. Returns: results ( dict ): Validation results. Contains the following: \"loss\" : Loss value. \"acc\" : Accuracy value. test_step(self, trainer, data, label, *args, **kwargs) Test step for standard supervised learning. Parameters: trainer (object): Trainer object. data (torch.Tensor): Input data. label (torch.Tensor): Label data. args (tuple): Additional arguments. kwargs (dict): Additional keyword arguments. Returns: results ( dict ): Test results. Contains the following: \"loss\" : Loss value. \"acc\" : Accuracy value. ncdia.dataloader ncdia.inference ncdia.model ncdia.trainers ncdia.utils","title":"ncdia"},{"location":"ncdia/#ncdia","text":"","title":"ncdia"},{"location":"ncdia/#ncdiaalgorithms","text":"Includes incremental learning algorithms, new class discovery algorithms, and out-of-distribution detection algorithms, as well as supervised learning algorithms.","title":"ncdia.algorithms"},{"location":"ncdia/#ncdiaalgorithmsbasepy","text":"","title":"ncdia.algorithms.base.py"},{"location":"ncdia/#basealg","text":"Basic algorithm class to define the interface of an algorithm. __init__(self, trainer) The constructor method that initializes an instance of BaseAlg . Parameters: trainer ( object ): Trainer object. train_step(self, trainer, data, label, *args, kwargs)** Training step. Parameters: trainer ( object ): Trainer object. data ( torch.Tensor ): Input data. label ( torch.Tensor ): Label data. args ( tuple ): Additional arguments. kwargs ( dict ): Additional keyword arguments. Returns: results ( dict ): Training results. Contains the following keys: \"loss\" : Loss value. \"acc\" : Accuracy value. other \"key:value\" pairs. val_step(self, trainer, data, label, *args, kwargs)** Validation step. Parameters: trainer ( object ): Trainer object. data ( torch.Tensor ): Input data. label ( torch.Tensor ): Label data. args ( tuple ): Additional arguments. kwargs ( dict ): Additional keyword arguments. Returns: results ( dict ): Validation results. Contains the following keys: \"loss\" : Loss value. \"acc\" : Accuracy value. other \"key:value\" pairs. test_step(self, trainer, data, label, *args, kwargs)** Test step. Parameters: trainer ( object ): Trainer object. data ( torch.Tensor ): Input data. label ( torch.Tensor ): Label data. args ( tuple ): Additional arguments. kwargs ( dict ): Additional keyword arguments. Returns: results ( dict ): Test results. Contains the following keys: \"loss\" : Loss value. \"acc\" : Accuracy value. other \"key:value\" pairs.","title":"BaseAlg"},{"location":"ncdia/#ncdiaalgorithmsincremental","text":"Include implementation of Class Incremental Learning (CIL) and Few-shot Class Incremental Learning (FSCIL) algorithm. CIL Finetune LwF EwC iCarL IL2A WA FSCIL ALICE FACT SAVC","title":"ncdia.algorithms.incremental"},{"location":"ncdia/#ncdiaalgorithmsncdautoncdpy","text":"Modules related to novel class discovery.","title":"ncdia.algorithms.ncd.autoncd.py"},{"location":"ncdia/#autoncd","text":"Class for evaluating with OOD metrics and relabeling the OOD dataset for the next session. __init__(self, model, train_loader, test_loader, device=None, verbose=False) The constructor method that initializes an instance of AutoNCD . Parameters: model ( nn.Module ): model to be evaluated. train_loader ( DataLoader ): train dataloader. test_loader ( DataLoader ): test dataloader. device ( torch.device, optional ): device to run the evaluation. Default to None. verbose ( bool, optional ): print the progress bar. Default to False. inference(self, dataloader, split='train') Inference the model on the dataloader and return relevant information. If split is 'train', return the prototype of the training data. Parameters: dataloader ( DataLoader ): dataloader for evaluation. split ( str, optional ): train or test. Defaults to 'train'. Returns: If split is 'train': features ( torch.Tensor ): feature vectors, (N, D). logits ( torch.Tensor ): logit vectors, (N, C). prototype_cls ( torch.Tensor ): prototype vectors, (C, D). If split is 'test': imgpaths ( list ): image paths (list). features ( torch.Tensor ): feature vectors, (N, D). logits ( torch.Tensor ): logit vectors, (N, C). preds ( torch.Tensor ): prediction labels, (N,). labels ( torch.Tensor ): ground truth labels, (N,). relabel(self, ood_loader, metrics=[], tpr_th=0.95, prec_th=None) Relabel the OOD dataset for the next session. Parameters: ood_loader ( DataLoader ): OOD dataloader for relabeling. metrics ( list, optional ): metrics to evaluate the OOD dataset. Defaults to []. tpr_th ( float, optional ): True positive rate threshold. Defaults to 0.95. prec_th ( float, optional ): Precision threshold. Defaults to None. Returns: ood_loader ( DataLoader ): relabeled OOD dataloader. _split_cluster_label(self, y_label, y_pred, ood_class) Calculate clustering accuracy. Require scikit-learn installed. First compute linear assignment on all data, then look at how good the accuracy is on subsets. Parameters: y_label ( numpy.array ): true labels, (n_samples,) y_pred ( numpy.array ): predicted labels (n_samples,) ood_class : out-of-distribution class labels Returns: cluster_label : cluster label search_discrete_point(self, novel_feat, novel_target) TODO","title":"AutoNCD"},{"location":"ncdia/#ncdiaalgorithmsoodautooodpy","text":"","title":"ncdia.algorithms.ood.autoood.py"},{"location":"ncdia/#autoood","text":"Class for evaluating OOD detection methods. eval(prototype_cls, fc_weight, train_feats, train_logits, id_feats, id_logits, id_labels, ood_feats, ood_logits, ood_labels, metrics=[], tpr_th=0.95, prec_th=None, id_attrs=None, ood_attrs=None, prototype_att=None) Evaluate the OOD detection methods and return OOD scores. Parameters: prototype_cls ( np.ndarray ): prototype of training data fc_weight ( np.ndarray ): weight of the last layer train_feats ( np.ndarray ): feature of training data train_logits ( np.ndarray ): logits of training data id_feats ( np.ndarray ): feature of ID data id_logits ( np.ndarray ): logits of ID data id_labels ( np.ndarray ): labels of ID data ood_feats ( np.ndarray ): feature of OOD data ood_logits ( np.ndarray ): logits of OOD data ood_labels ( np.ndarray ): labels of OOD data metrics ( list, optional ): list of OOD detection methods to evaluate. Defaults to []. tpr_th ( float, optional ): True positive rate threshold. Defaults to 0.95. prec_th ( float, optional ): Precision threshold. Defaults to None. Returns: ood_scores ( dict ): OOD scores, keys are the names of the OOD detection methods, values are the OOD scores and search threshold. Each value is a tuple containing the following: ood metrics ( tuple ): fpr ( float ): false positive rate auroc ( float ): area under the ROC curve aupr_in ( float ): area under the precision-recall curve for in-distribution samples aupr_out ( float ): area under the precision-recall curve for out-of-distribution samples search threshold ( tuple ): threshold for OOD detection if prec_th is not None best_th ( float ): best threshold for OOD detection conf ( torch.Tensor ): confidence scores label ( torch.Tensor ): label array precisions ( float ): precision when precisions >= prec_th recalls ( float ): recall when precisions >= prec_th inference(metrics, logits, feat, train_logits, train_feat, fc_weight, prototype, logits_att=None, prototype_att=None) Inferencec method for OOD detection Parameters: metrics ( list ): the ood metrics used for inference. logits ( np.ndarray ): logits of inference data. feat ( np.ndarray ): features of inference data. train_logits ( np.ndarray ): logits of training data. train_feat ( np.ndarray ): features of training data. fc_weight ( np.ndarray ): weight of the last layer. prototype ( np.ndarray ): prototypes of training data. logits_att ( np.ndarray, optional ): logits of attribute. prototype_att ( np.ndarray, optional ): prototypes of attribute. Returns: conf ( dict ): contains the confidence using different metrics, conf[metric] ( torch.Tensor ) is the confidence using specific metric.","title":"AutoOOD"},{"location":"ncdia/#ncdiaalgorithmsoodmethodspy","text":"msp(id_gt, id_logits, ood_gt, ood_logits, tpr_th=0.95, prec_th=None) Maximum Softmax Probability (MSP) method for OOD detection. A Baseline for Detecting Misclassified and Out-of-Distribution Examples in Neural Networks. Parameters: id_gt ( torch.Tensor ): ID ground truth labels. Shape (N,). id_logits ( torch.Tensor ): ID logits. Shape (N, C). ood_gt ( torch.Tensor ): OOD ground truth labels. Shape (M,). ood_logits ( torch.Tensor ): OOD logits. Shape (M, C). tpr_th ( float ): True positive rate threshold to compute false positive rate. Default is 0.95. prec_th ( float ): Precision threshold for searching - threshold. If None, not searching for threshold. Default is None. Returns: conf ( np.ndarray ): Confidence scores. Shape (N + M,). label ( np.ndarray ): Label array. Shape (N + M,). fpr ( float ): False positive rate. auroc ( float ): Area under the ROC curve. aupr_in ( float ): Area under the precision-recall curve for in-distribution samples. aupr_out ( float ): Area under the precision-recall curve for out-of-distribution best_th ( float ): Threshold for OOD detection. If prec_th is None, None. prec ( float ): Precision at the threshold. If prec_th is None, None. recall ( float ): Recall at the threshold. If prec_th is None, None. mcm(id_gt, id_logits, ood_gt, ood_logits, T=2, tpr_th=0.95, prec_th=None) Maximum Concept Matching (MCM) method for OOD detection. Delving into Out-of-Distribution Detection with Vision-Language Representations Parameters: id_gt ( torch.Tensor ): ID ground truth labels. Shape (N,). id_logits ( torch.Tensor ): ID logits. Shape (N, C). ood_gt ( torch.Tensor ): OOD ground truth labels. Shape (M,). ood_logits ( torch.Tensor ): OOD logits. Shape (M, C). T ( int ): Temperature for softmax. tpr_th ( float ): True positive rate threshold to compute false positive rate. Default is 0.95. prec_th ( float ): Precision threshold for searching threshold. If None, not searching for threshold. Default is None. Returns: fpr ( float ): False positive rate. auroc ( float ): Area under the ROC curve. aupr_in ( float ): Area under the precision-recall curve for in-distribution samples. aupr_out ( float ): Area under the precision-recall curve for out-of-distribution. max_logit(id_gt, id_logits, ood_gt, ood_logits, tpr_th=0.95, prec_th=None) Maximum Logit (MaxLogit) method for OOD detection. Scaling Out-of-Distribution Detection for Real-World Settings Parameters: id_gt ( torch.Tensor ): ID ground truth labels. Shape (N,). id_logits ( torch.Tensor ): ID logits. Shape (N, C). ood_gt ( torch.Tensor ): OOD ground truth labels. Shape (M,). ood_logits ( torch.Tensor ): OOD logits. Shape (M, C). tpr_th ( float ): True positive rate threshold to compute false positive rate. Default is 0.95. prec_th ( float ): Precision threshold for searching threshold. If None, not searching for threshold. Default is None. Returns: fpr ( float ): False positive rate. auroc ( float ): Area under the ROC curve. aupr_in ( float ): Area under the precision-recall curve for in-distribution samples. aupr_out ( float ): Area under the precision-recall curve for out-of-distribution energy(id_gt, id_logits, ood_gt, ood_logits, tpr_th=0.95, prec_th=None) Energy-based method for OOD detection. Energy-based Out-of-distribution Detection Parameters: id_gt ( torch.Tensor ): ID ground truth labels. Shape (N,). id_logits ( torch.Tensor ): ID logits. Shape (N, C). ood_gt ( torch.Tensor ): OOD ground truth labels. Shape (M,). ood_logits ( torch.Tensor ): OOD logits. Shape (M, C). tpr_th ( float ): True positive rate threshold to compute false positive rate. Default is 0.95. prec_th ( float ): Precision threshold for searching threshold. If None, not searching for threshold. Default is None. Returns: fpr ( float ): False positive rate. auroc ( float ): Area under the ROC curve. aupr_in ( float ): Area under the precision-recall curve for in-distribution samples. aupr_out ( float ): Area under the precision-recall curve for out-of-distribution vim(id_gt, id_logits, id_feat, ood_gt, ood_logits, ood_feat, train_logits, train_feat, tpr_th=0.95, prec_th=None) Virtual-Logit Matching (ViM) method for OOD detection. ViM: Out-of-Distribution With Virtual-Logit Matching Parameters: id_gt ( torch.Tensor ): ID ground truth labels. Shape (N,). id_logits ( torch.Tensor ): ID logits. Shape (N, C). id_feat ( torch.Tensor ): ID features. Shape (N, D). ood_gt ( torch.Tensor ): OOD ground truth labels. Shape (M,). ood_logits ( torch.Tensor ): OOD logits. Shape (M, C). ood_feat ( torch.Tensor ): OOD features. Shape (M, D). train_logits ( torch.Tensor ): Training logits. Shape (K, C). train_feat ( torch.Tensor ): Training features. Shape (K, D). tpr_th ( float ): True positive rate threshold to compute false positive rate. Default is 0.95. prec_th ( float ): Precision threshold for searching threshold. If None, not searching for threshold. Default is None. Returns: fpr ( float ): False positive rate. auroc ( float ): Area under the ROC curve. aupr_in ( float ): Area under the precision-recall curve for in-distribution samples. aupr_out ( float ): Area under the precision-recall curve for out-of-distribution dml(id_gt, id_feat, ood_gt, ood_feat, fc_weight, tpr_th=0.95, prec_th=None) Decoupled MaxLogit (DML) method for OOD detection. Decoupling MaxLogit for Out-of-Distribution Detection Parameters: id_gt ( torch.Tensor ): ID ground truth labels. Shape (N,). id_feat ( torch.Tensor ): ID features. Shape (N, D). ood_gt ( torch.Tensor ): OOD ground truth labels. Shape (M,). ood_feat ( torch.Tensor ): OOD features. Shape (M, D). fc_weight ( torch.Tensor ): FC layer weight. Shape (C, D). tpr_th ( float ): True positive rate threshold to compute false positive rate. Default is 0.95. prec_th ( float ): Precision threshold for searching threshold. If None, not searching for threshold. Default is None. Returns: fpr ( float ): False positive rate. auroc ( float ): Area under the ROC curve. aupr_in ( float ): Area under the precision-recall curve for in-distribution samples. aupr_out ( float ): Area under the precision-recall curve for out-of-distribution dmlp(id_gt, id_logits, id_feat, ood_gt, ood_logits, ood_feat, fc_weight, prototype,tpr_th=0.95, prec_th=None) Decoupled MaxLogit+ (DML+) method for OOD detection. Decoupling MaxLogit for Out-of-Distribution Detection Parameters: id_gt ( torch.Tensor ): ID ground truth labels. Shape (N,). id_logits ( torch.Tensor ): ID logits. Shape (N, C). id_feat ( torch.Tensor ): ID features. Shape (N, D). ood_gt ( torch.Tensor ): OOD ground truth labels. Shape (M,). ood_logits ( torch.Tensor ): OOD logits. Shape (M, C). ood_feat ( torch.Tensor ): OOD features. Shape (M, D). fc_weight ( torch.Tensor ): FC layer weight. Shape (D, C). prototype ( torch.Tensor ): Prototype. Shape (D, C). tpr_th ( float ): True positive rate threshold to compute false positive rate. Default is 0.95. prec_th ( float ): Precision threshold for searching threshold. If None, not searching for threshold. Default is None. Returns: fpr ( float ): False positive rate. auroc ( float ): Area under the ROC curve. aupr_in ( float ): Area under the precision-recall curve for in-distribution samples. aupr_out ( float ): Area under the precision-recall curve for out-of-distribution prot(id_gt, id_logits, ood_gt, ood_logits, prototypes: list, tpr_th=0.95, prec_th=None) Prototype-based (Prot) method for OOD detection. Parameters: id_gt ( torch.Tensor ): ID ground truth labels, shape (N,). id_logits ( list of torch.Tensor ): ID logits, containing shape (N, C). ood_gt ( torch.Tensor ): OOD ground truth labels, shape (M,). ood_logits ( list of torch.Tensor ): OOD logits, containing shape (M, C). prototypes ( list of torch.Tensor ): Prototypes, containing shape (D, C). tpr_th ( float ): True positive rate threshold to compute false positive rate. prec_th ( float ): Precision threshold for searching threshold. If None, not searching for threshold. Default is Returns: fpr ( float ): False positive rate. auroc ( float ): Area under the ROC curve. aupr_in ( float ): Area under the precision-recall curve for in-distribution samples. aupr_out ( float ): Area under the precision-recall curve for out-of-distribution","title":"ncdia.algorithms.ood.methods.py"},{"location":"ncdia/#ncdiaalgorithmsoodinferencepy","text":"The inference version of implemented ood methods in ncdia.algorithms.ood.methods.py","title":"ncdia.algorithms.ood.inference.py"},{"location":"ncdia/#ncdiaalgorithmsoodmetricspy","text":"ood_metrics(conf, label, tpr_th=0.95) Compute OOD metrics. Parameters: conf ( np.ndarray ): Confidence scores. Shape (N,). label ( np.ndarray ): Label array. Shape (N,). Containing: -1: OOD samples. int >= 0: ID samples with class labels tpr_th ( float ): True positive rate threshold to compute false positive rate. Returns: fpr ( float ): False positive rate. auroc ( float ): Area under the ROC curve. aupr_in ( float ): Area under the precision-recall curve for in-distribution samples. aupr_out ( float ): Area under the precision-recall curve for out-of-distribution samples. search_threshold(conf, label, prec_th) Search for the threshold for OOD detection. Parameters: conf ( np.ndarray ): Confidence scores. Shape (N,). label ( np.ndarray ): Label array. Shape (N,). Containing: -1: OOD samples. int >= 0: ID samples with class labels prec_th ( float ): Precision threshold. Returns: best_th ( float ): Threshold for OOD detection. prec ( float ): Precision at the threshold. recall ( float ): Recall at the threshold.","title":"ncdia.algorithms.ood.metrics.py"},{"location":"ncdia/#ncdiaalgorithmssupervisedstandardpy","text":"Modules related to supervised learning","title":"ncdia.algorithms.supervised.standard.py"},{"location":"ncdia/#standardsl","text":"Class inherits from BaseAlg . Standard supervised learning algorithm train_step(self, trainer, data, label, *args, **kwargs) Training step for standard supervised learning. Parameters: trainer ( object ): Trainer object. data ( torch.Tensor ): Input data. label ( torch.Tensor ): Label data. args ( tuple ): Additional arguments. kwargs ( dict ): Additional keyword arguments. Returns: results ( dict ): Training results. Contains the following keys: \"loss\" : Loss value. \"acc\" : Accuracy value. val_step(self, trainer, data, label, *args, **kwargs) Validation step for standard supervised learning. Parameters: trainer ( object ): Trainer object. data ( torch.Tensor ): Input data. label ( torch.Tensor ): Label data. args ( tuple ): Additional arguments. kwargs ( dict ): Additional keyword arguments. Returns: results ( dict ): Validation results. Contains the following: \"loss\" : Loss value. \"acc\" : Accuracy value. test_step(self, trainer, data, label, *args, **kwargs) Test step for standard supervised learning. Parameters: trainer (object): Trainer object. data (torch.Tensor): Input data. label (torch.Tensor): Label data. args (tuple): Additional arguments. kwargs (dict): Additional keyword arguments. Returns: results ( dict ): Test results. Contains the following: \"loss\" : Loss value. \"acc\" : Accuracy value.","title":"StandardSL"},{"location":"ncdia/#ncdiadataloader","text":"","title":"ncdia.dataloader"},{"location":"ncdia/#ncdiainference","text":"","title":"ncdia.inference"},{"location":"ncdia/#ncdiamodel","text":"","title":"ncdia.model"},{"location":"ncdia/#ncdiatrainers","text":"","title":"ncdia.trainers"},{"location":"ncdia/#ncdiautils","text":"","title":"ncdia.utils"},{"location":"overview/","text":"OpenHAIV Documentation The framework adopts a modular design overall, which is reflected in two key aspects: Functionally , it independently incorporates dedicated modules for supervised training, out-of-distribution detection, novel class discovery, and incremental learning. Procedurally , the framework divides its operational workflow into distinct stages, including data processing, model construction, training \\& evaluation, and visualization. From an implementation perspective, the framework is built upon foundational deep learning, data processing, and visualization libraries such as PyTorch, NumPy, Matplotlib, and Pandas, leveraging their extensive built-in functionalities.","title":"Overview"},{"location":"overview/#openhaiv-documentation","text":"The framework adopts a modular design overall, which is reflected in two key aspects: Functionally , it independently incorporates dedicated modules for supervised training, out-of-distribution detection, novel class discovery, and incremental learning. Procedurally , the framework divides its operational workflow into distinct stages, including data processing, model construction, training \\& evaluation, and visualization. From an implementation perspective, the framework is built upon foundational deep learning, data processing, and visualization libraries such as PyTorch, NumPy, Matplotlib, and Pandas, leveraging their extensive built-in functionalities.","title":"OpenHAIV Documentation"},{"location":"people/","text":"","title":"People"},{"location":"quik/","text":"Install It is recommended to use anaconda3 to manage and maintain the python library environment. 1. Download the .sh file from the anaconda3 website 2. install anaconda3 with .sh file bash Anaconda3-2023.03-Linux-x86_64.sh create virtual environment conda create -n ncdia python=3.10 -y conda activate ncdia pip install -r requirements.txt python setup.py install install package pytorch>=1.12.0 torchvision>=0.13.0 (recommand offical torch command) numpy>=1.26.4 scipy>=1.14.0 scikit-learn>=1.5.1 Run OOD Example: bash ./scripts/det_oes_rn18_msp.sh CIL Example: bash ./scripts/inc_BM200_lwf.sh","title":"Quik Start"},{"location":"quik/#install","text":"It is recommended to use anaconda3 to manage and maintain the python library environment. 1. Download the .sh file from the anaconda3 website 2. install anaconda3 with .sh file bash Anaconda3-2023.03-Linux-x86_64.sh","title":"Install"},{"location":"quik/#create-virtual-environment","text":"conda create -n ncdia python=3.10 -y conda activate ncdia pip install -r requirements.txt python setup.py install","title":"create virtual environment"},{"location":"quik/#install-package","text":"pytorch>=1.12.0 torchvision>=0.13.0 (recommand offical torch command) numpy>=1.26.4 scipy>=1.14.0 scikit-learn>=1.5.1","title":"install package"},{"location":"quik/#run","text":"","title":"Run"},{"location":"quik/#ood","text":"Example: bash ./scripts/det_oes_rn18_msp.sh","title":"OOD"},{"location":"quik/#cil","text":"Example: bash ./scripts/inc_BM200_lwf.sh","title":"CIL"},{"location":"results/","text":"","title":"Results"},{"location":"scripts/","text":"scripts","title":"scripts"},{"location":"scripts/#scripts","text":"","title":"scripts"}]}